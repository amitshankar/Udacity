{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "https://stackoverflow.com/questions/14325654/how-to-use-python-markdown-to-process-a-file-that-is-read-in\n",
    "https://stackoverflow.com/questions/3207219/how-do-i-list-all-files-of-a-directory\n",
    "https://stackoverflow.com/questions/14560863/python-join-with-newline\n",
    "https://stackoverflow.com/questions/743806/how-to-split-a-string-into-a-list\n",
    "https://stackabuse.com/reading-and-writing-lists-to-a-file-in-python/\n",
    "https://stackoverflow.com/questions/6996603/delete-a-file-or-folder\n",
    "https://stackoverflow.com/questions/5214578/python-print-string-to-text-file\n",
    "https://stackoverflow.com/questions/89228/calling-an-external-command-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os #to run commands in command prompt in python\n",
    "import glob # to get paths to files in a folder\n",
    "import shutil #copy files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        print('Creating Directory '+directory)\n",
    "        os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "folder structure\n",
    "\n",
    "clean_md_file.ipynb\n",
    "project_03_01.ipynb\n",
    "data>>\n",
    "git_page>>project_03_01.md\n",
    "git_page>>images>>movie_data_eda>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrangle_report_v2_0.ipynb\n",
      "data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0.md\n",
      "data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0_files\n",
      "data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0\n"
     ]
    }
   ],
   "source": [
    "#inputs\n",
    "#enter the ipython notebook name\n",
    "#notebook_name='project_03_01.ipynb'\n",
    "#markdown_name='project_03_01.md'\n",
    "#images_folder_name=markdown_name.replace('.md','')+'_files' #remove the .md and add _files\n",
    "#subfolder_name='project_03_01' # images/'subfolder_name'/ -- this will contain all images\n",
    "\n",
    "#data_analyst_nanodegree_term_02_project_01_perceptual_phenomenon_python_004\n",
    "#data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0\n",
    "notebook_name='wrangle_report_v2_0.ipynb'\n",
    "markdown_name='data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0.md'\n",
    "images_folder_name=markdown_name.replace('.md','')+'_files' #remove the .md and add _files\n",
    "subfolder_name='data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0' # images/'subfolder_name'/ -- this will contain all images\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(notebook_name) # name of jupyter notebook\n",
    "print(markdown_name) #md file made during jupyter to md conversion, moved to git_page folder and deleted from root\n",
    "print(images_folder_name) #default folder made when generating md file - later deleted\n",
    "print(subfolder_name) #name of the subfolder inside images subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Directory git_page/images/data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0\n"
     ]
    }
   ],
   "source": [
    "#create the directory structure\n",
    "create_dir('git_page/images/'+subfolder_name) #subfolder for all project images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jupyter nbconvert wrangle_report_v2_0.ipynb --to markdown --output data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0.md\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert project_03_01.ipynb --to markdown --output test.md\n",
    "## first create a string using the notebook name and markdown name\n",
    "string_01='jupyter nbconvert '+ notebook_name + ' --to markdown --output '+ markdown_name\n",
    "#string_01='jupyter nbconvert '+ notebook_name + ' --to markdown' \n",
    "\n",
    "print(string_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#run the string_01 in command prompt to generate the markdown files\n",
    "os.system(string_01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get a list of only image names in sub directory\n",
    "#image_links = [f for f in glob.glob(\"markdown\\*.png\")]\n",
    "\n",
    "images_path=images_folder_name+'\\*.png'\n",
    "image_links = [f for f in glob.glob(images_path)]\n",
    "\n",
    "image_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from the image_links, only save the image names\n",
    "#save just the image names to a list\n",
    "image_names=[]\n",
    "for i in image_links:\n",
    "    #image_names.append(i.replace('markdown\\\\',''))\n",
    "    image_names.append(i.replace(images_folder_name+'\\\\',''))\n",
    "    \n",
    "image_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<!â€“â€“ The codes in this cell will add a toggle button to show/hide jupyter codes in web browser  â€“â€“>\\n',\n",
       " '<!â€“â€“ Source: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer  â€“â€“>\\n',\n",
       " '\\n',\n",
       " '<script>\\n',\n",
       " '  function code_toggle() {\\n',\n",
       " '    if (code_shown){\\n',\n",
       " \"      $('div.input').hide('500');\\n\",\n",
       " \"      $('#toggleButton').val('Show Code')\\n\",\n",
       " '    } else {\\n',\n",
       " \"      $('div.input').show('500');\\n\",\n",
       " \"      $('#toggleButton').val('Hide Code')\\n\",\n",
       " '    }\\n',\n",
       " '    code_shown = !code_shown\\n',\n",
       " '  }\\n',\n",
       " '\\n',\n",
       " '  $( document ).ready(function(){\\n',\n",
       " '    code_shown=false;\\n',\n",
       " \"    $('div.input').hide()\\n\",\n",
       " '  });\\n',\n",
       " '</script>\\n',\n",
       " '<form action=\"javascript:code_toggle()\">\\n',\n",
       " '  <input type=\"submit\" id=\"toggleButton\" value=\"Show Code\">\\n',\n",
       " '</form>\\n',\n",
       " '# Wrangle Report\\n',\n",
       " '\\n',\n",
       " '## 1. Data Gathering Efforts \\n',\n",
       " '\\n',\n",
       " 'The twitter-archive-enhanced.csv was saved to the local directory by clicking the following link https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv and saving the corresponding csv file in the local working directory. This was a manual process and not scaleable. This process could be made more efficient by directly passing the url to the pandas read csv function eg. \\n',\n",
       " \"pd.read_csv('https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv').\\n\",\n",
       " '\\n',\n",
       " \"The image_predictions.tsv was programmatically downloaded from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv using the requests funciton in conjunction with file.write function. This is very scaleable becuase it allows for setting dynamic filenames by extracting file names from the url and saving the content to specific file name. The mode='wb' allows for saving files individually but by setting mode='a', we can append content from numerous files into one file.\\n\",\n",
       " '\\n',\n",
       " 'Downloading Twitter data via Twitter API and tweepy library was a lengthy learning process. First set the consumer and access token/keys. Used a json parser by setting tweepy.API(parser=tweepy.parsers.JSONParser()) becuase twitter data is stored in json format and the json parser allows for converting to downloaded data in json format. Once the data is in json format, it is appended to  tweet_json.txt file in the working directory. After the data is saved to working directory, it is then read back into memory as a python list. Each element in the list is converted back into a json format and revelant data is extracted such as tweet id, favorites count and retweet count.\\n',\n",
       " '\\n',\n",
       " \"It is helpful to save each tweet's complete data to the file and read it back to extract info because it saves time in the future to re-read the data from file than to read the data from twitter's website.\\n\",\n",
       " '\\n',\n",
       " \"api.get_status(tweet_mode='extended')\\n\",\n",
       " \"By setting the tweet_mode='extended', allows the api to download more data relating to a tweet. I have noticed that the tweet text is truncated followed by a '...' if the tweet_mode is not set to extended .i.e. in extended mode, the api is able to download the complete tweet text. \\n\",\n",
       " '\\n',\n",
       " '**Some important takeaways:** \\n',\n",
       " \"1. It is helpful to name dataframes with extenion df_***, so typing a dataframe name, we can type 'df' and autocomplete can show a drop down list of dataframes for us to choose from.\\n\",\n",
       " '2. It is also helpful to paste the json format tweet in sublime text to see the indents in the text and this is very helpful when extracting  values from multilevel lists.\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '## 2. Data Accessing Issues \\n',\n",
       " '\\n',\n",
       " '## Content Issues \\n',\n",
       " '\\n',\n",
       " '(Visual & Programmatically - completness, validity, accuracy, consistency)\\n',\n",
       " '\\n',\n",
       " '### twitter_archive dataframe\\n',\n",
       " '    \\n',\n",
       " '    - The expanded_url column, has some instances where the url is repeated multiple times in a cell separated by a comma\\n',\n",
       " '    - The name column has non name strings such as None, a, an \\n',\n",
       " '    - Rating_denominator as high as 80\\n',\n",
       " '    - The following variables should be integers instead of floats: in_reply_to_status_id,in_reply_to_user_id,    \\n',\n",
       " '      retweeted_status_id,retweeted_status_user_id \\n',\n",
       " '    - Contains retweets\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '### image_predictions dataframe\\n',\n",
       " '\\n',\n",
       " '    - p1, p2,p3: upper and lower case mixed together\\n',\n",
       " '    - p1, p2,p3: dash and underscore mixed in string eg. black-and-tan_coonhound\\t\\n',\n",
       " '    - Missing values when compared to twitter_archive dataframe\\n',\n",
       " '\\n',\n",
       " '###  tweet_json dataframe\\n',\n",
       " '\\n',\n",
       " '    - time_created should be a data/time object\\n',\n",
       " '\\n',\n",
       " '## Structural Issues\\n',\n",
       " '\\n',\n",
       " '(Visual & Programmatically -  variable(column), observation(row), unit(table))\\n',\n",
       " '\\n',\n",
       " '### twitter_archive dataframe\\n',\n",
       " '\\n',\n",
       " '    - The timestamp column contains two separate variables date and time. \\n',\n",
       " '    - The anchor text in source column is repeated numerous times\\n',\n",
       " \"    - Variables called 'doggo', 'floofer', 'pupper', 'puppo' are different growth stages of a pet based on age.\\n\",\n",
       " '\\n',\n",
       " '### image_predictions dataframe\\n',\n",
       " '\\n',\n",
       " '    - Merge the dataset with twitter_archive dataframe based on tweet_id\\n',\n",
       " '\\n',\n",
       " '###  tweet_json dataframe\\n',\n",
       " '\\n',\n",
       " '    - time_created could be further split into day, month and time \\n',\n",
       " '    - Merge this dataframe with twitter_archive dataframe\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '## 3. Data Cleaning \\n',\n",
       " '\\n',\n",
       " 'Cleaning the timestamp variable in df_twitter_archive_clean dataframe was tricky. The pd.to_datetime function was very helpful in converting a column to datetime format. The month, week and hour function in the datetime library can not be applied directly to a dataframe column and mapping using lambda function was very helpful. \\n',\n",
       " '\\n',\n",
       " 'Also mapping the dictory on a dataframe column is very helpful instead of using a multilevel if else condition. I used dictionary to map source values in the df_twitter_archive_clean dataframe.\\n',\n",
       " '\\n',\n",
       " 'Its also useful to note that a print function on a text column with truncate the results on screen and mapping a lambda function works well eg.  df_twitter_archive_clean.expanded_urls.map(lambda a : print(a))\\n',\n",
       " '\\n',\n",
       " '#### Goal\\n',\n",
       " '\\n',\n",
       " 'The goal of cleaning was to produce a high quality dataframe that can provide insights to these questions: \\n',\n",
       " '\\n',\n",
       " '1. Is the tweet that received the most favorites count also the tweet that was retweeted most?\\n',\n",
       " '2. What day of the week were most of the tweets created? \\n',\n",
       " '3. What are some of the common words used in the top tweets?\\n',\n",
       " '\\n',\n",
       " '\\n',\n",
       " '```python\\n',\n",
       " '\\n',\n",
       " '```\\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open the file with read only permit\n",
    "f = open(markdown_name, \"r\")\n",
    "# use readlines to read all lines in the file\n",
    "# The variable \"lines\" is a list containing all lines in the file\n",
    "lines = f.readlines()\n",
    "# close the file after reading the lines.\n",
    "f.close()\n",
    "\n",
    "lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the all the element sin lines list witn @@@ at the end of each line to perform operations\n",
    "content_joined=\"@@@\".join(lines)\n",
    "\n",
    "#use this below to split at @@@ and then save it to a md file\n",
    "#content_joined.split('@@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!â€“â€“ The codes in this cell will add a toggle button to show/hide jupyter codes in web browser  â€“â€“>\\n@@@<!â€“â€“ Source: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer  â€“â€“>\\n@@@\\n@@@<script>\\n@@@  function code_toggle() {\\n@@@    if (code_shown){\\n@@@      $(\\'div.input\\').hide(\\'500\\');\\n@@@      $(\\'#toggleButton\\').val(\\'Show Code\\')\\n@@@    } else {\\n@@@      $(\\'div.input\\').show(\\'500\\');\\n@@@      $(\\'#toggleButton\\').val(\\'Hide Code\\')\\n@@@    }\\n@@@    code_shown = !code_shown\\n@@@  }\\n@@@\\n@@@  $( document ).ready(function(){\\n@@@    code_shown=false;\\n@@@    $(\\'div.input\\').hide()\\n@@@  });\\n@@@</script>\\n@@@<form action=\"javascript:code_toggle()\">\\n@@@  <input type=\"submit\" id=\"toggleButton\" value=\"Show Code\">\\n@@@</form>\\n@@@# Wrangle Report\\n@@@\\n@@@## 1. Data Gathering Efforts \\n@@@\\n@@@The twitter-archive-enhanced.csv was saved to the local directory by clicking the following link https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv and saving the corresponding csv file in the local working directory. This was a manual process and not scaleable. This process could be made more efficient by directly passing the url to the pandas read csv function eg. \\n@@@pd.read_csv(\\'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\\').\\n@@@\\n@@@The image_predictions.tsv was programmatically downloaded from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv using the requests funciton in conjunction with file.write function. This is very scaleable becuase it allows for setting dynamic filenames by extracting file names from the url and saving the content to specific file name. The mode=\\'wb\\' allows for saving files individually but by setting mode=\\'a\\', we can append content from numerous files into one file.\\n@@@\\n@@@Downloading Twitter data via Twitter API and tweepy library was a lengthy learning process. First set the consumer and access token/keys. Used a json parser by setting tweepy.API(parser=tweepy.parsers.JSONParser()) becuase twitter data is stored in json format and the json parser allows for converting to downloaded data in json format. Once the data is in json format, it is appended to  tweet_json.txt file in the working directory. After the data is saved to working directory, it is then read back into memory as a python list. Each element in the list is converted back into a json format and revelant data is extracted such as tweet id, favorites count and retweet count.\\n@@@\\n@@@It is helpful to save each tweet\\'s complete data to the file and read it back to extract info because it saves time in the future to re-read the data from file than to read the data from twitter\\'s website.\\n@@@\\n@@@api.get_status(tweet_mode=\\'extended\\')\\n@@@By setting the tweet_mode=\\'extended\\', allows the api to download more data relating to a tweet. I have noticed that the tweet text is truncated followed by a \\'...\\' if the tweet_mode is not set to extended .i.e. in extended mode, the api is able to download the complete tweet text. \\n@@@\\n@@@**Some important takeaways:** \\n@@@1. It is helpful to name dataframes with extenion df_***, so typing a dataframe name, we can type \\'df\\' and autocomplete can show a drop down list of dataframes for us to choose from.\\n@@@2. It is also helpful to paste the json format tweet in sublime text to see the indents in the text and this is very helpful when extracting  values from multilevel lists.\\n@@@\\n@@@\\n@@@\\n@@@## 2. Data Accessing Issues \\n@@@\\n@@@## Content Issues \\n@@@\\n@@@(Visual & Programmatically - completness, validity, accuracy, consistency)\\n@@@\\n@@@### twitter_archive dataframe\\n@@@    \\n@@@    - The expanded_url column, has some instances where the url is repeated multiple times in a cell separated by a comma\\n@@@    - The name column has non name strings such as None, a, an \\n@@@    - Rating_denominator as high as 80\\n@@@    - The following variables should be integers instead of floats: in_reply_to_status_id,in_reply_to_user_id,    \\n@@@      retweeted_status_id,retweeted_status_user_id \\n@@@    - Contains retweets\\n@@@\\n@@@\\n@@@### image_predictions dataframe\\n@@@\\n@@@    - p1, p2,p3: upper and lower case mixed together\\n@@@    - p1, p2,p3: dash and underscore mixed in string eg. black-and-tan_coonhound\\t\\n@@@    - Missing values when compared to twitter_archive dataframe\\n@@@\\n@@@###  tweet_json dataframe\\n@@@\\n@@@    - time_created should be a data/time object\\n@@@\\n@@@## Structural Issues\\n@@@\\n@@@(Visual & Programmatically -  variable(column), observation(row), unit(table))\\n@@@\\n@@@### twitter_archive dataframe\\n@@@\\n@@@    - The timestamp column contains two separate variables date and time. \\n@@@    - The anchor text in source column is repeated numerous times\\n@@@    - Variables called \\'doggo\\', \\'floofer\\', \\'pupper\\', \\'puppo\\' are different growth stages of a pet based on age.\\n@@@\\n@@@### image_predictions dataframe\\n@@@\\n@@@    - Merge the dataset with twitter_archive dataframe based on tweet_id\\n@@@\\n@@@###  tweet_json dataframe\\n@@@\\n@@@    - time_created could be further split into day, month and time \\n@@@    - Merge this dataframe with twitter_archive dataframe\\n@@@\\n@@@\\n@@@## 3. Data Cleaning \\n@@@\\n@@@Cleaning the timestamp variable in df_twitter_archive_clean dataframe was tricky. The pd.to_datetime function was very helpful in converting a column to datetime format. The month, week and hour function in the datetime library can not be applied directly to a dataframe column and mapping using lambda function was very helpful. \\n@@@\\n@@@Also mapping the dictory on a dataframe column is very helpful instead of using a multilevel if else condition. I used dictionary to map source values in the df_twitter_archive_clean dataframe.\\n@@@\\n@@@Its also useful to note that a print function on a text column with truncate the results on screen and mapping a lambda function works well eg.  df_twitter_archive_clean.expanded_urls.map(lambda a : print(a))\\n@@@\\n@@@#### Goal\\n@@@\\n@@@The goal of cleaning was to produce a high quality dataframe that can provide insights to these questions: \\n@@@\\n@@@1. Is the tweet that received the most favorites count also the tweet that was retweeted most?\\n@@@2. What day of the week were most of the tweets created? \\n@@@3. What are some of the common words used in the top tweets?\\n@@@\\n@@@\\n@@@```python\\n@@@\\n@@@```\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backup for test purposes\n",
    "tmp=content_joined\n",
    "#content_joined=tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old image link\n",
    "#project_03_01_files/project_03_01_13_0.png\n",
    "\n",
    "#new image link\n",
    "#images/project_03_01_files/project_03_01_13_0.png\n",
    "\n",
    "#updated_image_link='images/'+subfolder_name+'/'+image_names[0]\n",
    "#content_joined.replace(images_folder_name+'/'+image_names[0],updated_image_link)\n",
    "\n",
    "\n",
    "#replace the image names with proper links+names\n",
    "for i in image_names:\n",
    "    #generate new links for md file\n",
    "    updated_image_link='images/'+subfolder_name+'/'+i\n",
    "    #replace the image name with proper image link\n",
    "    content_joined=content_joined.replace(images_folder_name+'/'+i,updated_image_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!â€“â€“ The codes in this cell will add a toggle button to show/hide jupyter codes in web browser  â€“â€“>\\n@@@<!â€“â€“ Source: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer  â€“â€“>\\n@@@\\n@@@<script>\\n@@@  function code_toggle() {\\n@@@    if (code_shown){\\n@@@      $(\\'div.input\\').hide(\\'500\\');\\n@@@      $(\\'#toggleButton\\').val(\\'Show Code\\')\\n@@@    } else {\\n@@@      $(\\'div.input\\').show(\\'500\\');\\n@@@      $(\\'#toggleButton\\').val(\\'Hide Code\\')\\n@@@    }\\n@@@    code_shown = !code_shown\\n@@@  }\\n@@@\\n@@@  $( document ).ready(function(){\\n@@@    code_shown=false;\\n@@@    $(\\'div.input\\').hide()\\n@@@  });\\n@@@</script>\\n@@@<form action=\"javascript:code_toggle()\">\\n@@@  <input type=\"submit\" id=\"toggleButton\" value=\"Show Code\">\\n@@@</form>\\n@@@# Wrangle Report\\n@@@\\n@@@## 1. Data Gathering Efforts \\n@@@\\n@@@The twitter-archive-enhanced.csv was saved to the local directory by clicking the following link https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv and saving the corresponding csv file in the local working directory. This was a manual process and not scaleable. This process could be made more efficient by directly passing the url to the pandas read csv function eg. \\n@@@pd.read_csv(\\'https://d17h27t6h515a5.cloudfront.net/topher/2017/August/59a4e958_twitter-archive-enhanced/twitter-archive-enhanced.csv\\').\\n@@@\\n@@@The image_predictions.tsv was programmatically downloaded from https://d17h27t6h515a5.cloudfront.net/topher/2017/August/599fd2ad_image-predictions/image-predictions.tsv using the requests funciton in conjunction with file.write function. This is very scaleable becuase it allows for setting dynamic filenames by extracting file names from the url and saving the content to specific file name. The mode=\\'wb\\' allows for saving files individually but by setting mode=\\'a\\', we can append content from numerous files into one file.\\n@@@\\n@@@Downloading Twitter data via Twitter API and tweepy library was a lengthy learning process. First set the consumer and access token/keys. Used a json parser by setting tweepy.API(parser=tweepy.parsers.JSONParser()) becuase twitter data is stored in json format and the json parser allows for converting to downloaded data in json format. Once the data is in json format, it is appended to  tweet_json.txt file in the working directory. After the data is saved to working directory, it is then read back into memory as a python list. Each element in the list is converted back into a json format and revelant data is extracted such as tweet id, favorites count and retweet count.\\n@@@\\n@@@It is helpful to save each tweet\\'s complete data to the file and read it back to extract info because it saves time in the future to re-read the data from file than to read the data from twitter\\'s website.\\n@@@\\n@@@api.get_status(tweet_mode=\\'extended\\')\\n@@@By setting the tweet_mode=\\'extended\\', allows the api to download more data relating to a tweet. I have noticed that the tweet text is truncated followed by a \\'...\\' if the tweet_mode is not set to extended .i.e. in extended mode, the api is able to download the complete tweet text. \\n@@@\\n@@@**Some important takeaways:** \\n@@@1. It is helpful to name dataframes with extenion df_***, so typing a dataframe name, we can type \\'df\\' and autocomplete can show a drop down list of dataframes for us to choose from.\\n@@@2. It is also helpful to paste the json format tweet in sublime text to see the indents in the text and this is very helpful when extracting  values from multilevel lists.\\n@@@\\n@@@\\n@@@\\n@@@## 2. Data Accessing Issues \\n@@@\\n@@@## Content Issues \\n@@@\\n@@@(Visual & Programmatically - completness, validity, accuracy, consistency)\\n@@@\\n@@@### twitter_archive dataframe\\n@@@    \\n@@@    - The expanded_url column, has some instances where the url is repeated multiple times in a cell separated by a comma\\n@@@    - The name column has non name strings such as None, a, an \\n@@@    - Rating_denominator as high as 80\\n@@@    - The following variables should be integers instead of floats: in_reply_to_status_id,in_reply_to_user_id,    \\n@@@      retweeted_status_id,retweeted_status_user_id \\n@@@    - Contains retweets\\n@@@\\n@@@\\n@@@### image_predictions dataframe\\n@@@\\n@@@    - p1, p2,p3: upper and lower case mixed together\\n@@@    - p1, p2,p3: dash and underscore mixed in string eg. black-and-tan_coonhound\\t\\n@@@    - Missing values when compared to twitter_archive dataframe\\n@@@\\n@@@###  tweet_json dataframe\\n@@@\\n@@@    - time_created should be a data/time object\\n@@@\\n@@@## Structural Issues\\n@@@\\n@@@(Visual & Programmatically -  variable(column), observation(row), unit(table))\\n@@@\\n@@@### twitter_archive dataframe\\n@@@\\n@@@    - The timestamp column contains two separate variables date and time. \\n@@@    - The anchor text in source column is repeated numerous times\\n@@@    - Variables called \\'doggo\\', \\'floofer\\', \\'pupper\\', \\'puppo\\' are different growth stages of a pet based on age.\\n@@@\\n@@@### image_predictions dataframe\\n@@@\\n@@@    - Merge the dataset with twitter_archive dataframe based on tweet_id\\n@@@\\n@@@###  tweet_json dataframe\\n@@@\\n@@@    - time_created could be further split into day, month and time \\n@@@    - Merge this dataframe with twitter_archive dataframe\\n@@@\\n@@@\\n@@@## 3. Data Cleaning \\n@@@\\n@@@Cleaning the timestamp variable in df_twitter_archive_clean dataframe was tricky. The pd.to_datetime function was very helpful in converting a column to datetime format. The month, week and hour function in the datetime library can not be applied directly to a dataframe column and mapping using lambda function was very helpful. \\n@@@\\n@@@Also mapping the dictory on a dataframe column is very helpful instead of using a multilevel if else condition. I used dictionary to map source values in the df_twitter_archive_clean dataframe.\\n@@@\\n@@@Its also useful to note that a print function on a text column with truncate the results on screen and mapping a lambda function works well eg.  df_twitter_archive_clean.expanded_urls.map(lambda a : print(a))\\n@@@\\n@@@#### Goal\\n@@@\\n@@@The goal of cleaning was to produce a high quality dataframe that can provide insights to these questions: \\n@@@\\n@@@1. Is the tweet that received the most favorites count also the tweet that was retweeted most?\\n@@@2. What day of the week were most of the tweets created? \\n@@@3. What are some of the common words used in the top tweets?\\n@@@\\n@@@\\n@@@```python\\n@@@\\n@@@```\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_joined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use this below to split at @@@ and then save it to a md file\n",
    "lines=content_joined.split('@@@')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use this to write the files back\n",
    "with open(markdown_name, 'w') as filehandle:  \n",
    "    filehandle.writelines(\"%s\" % place for place in lines)\n",
    "    #code below will add a line break\n",
    "    #filehandle.writelines(\"%s\\n\" % place for place in places_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-a795cf9fc51d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#define source and destination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_folder_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdestination\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'git_page/images/'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0msubfolder_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0_files'"
     ]
    }
   ],
   "source": [
    "#copy image files to the images/movie_data_eda subfoler\n",
    "\n",
    "#define source and destination\n",
    "source=os.listdir(images_folder_name)\n",
    "destination='git_page/images/'+subfolder_name\n",
    "\n",
    "\n",
    "#works\n",
    "for files in source:\n",
    "    if files.endswith(\".png\"):\n",
    "        shutil.copy(images_folder_name+'/'+files,destination)\n",
    "        #print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'git_page/data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0.md'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#copy md file to git_page folder\n",
    "#source='markdown/project_03_01.md'\n",
    "#destination='git_page/'\n",
    "#shutil.copy(source, destination)\n",
    "\n",
    "source=markdown_name\n",
    "destination='git_page/'\n",
    "shutil.copy(source, destination)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0_files'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-941f67ec5233>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#delete the images folder that is OUTSIDE the git_page folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_folder_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#delete the markdonw file that is OUTSIDE the git_page folder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmarkdown_name\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#remove file\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36mrmtree\u001b[1;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 494\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_rmtree_unsafe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monerror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    495\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;31m# Allow introspection of whether or not the hardening against symlink\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[0mfullname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\shutil.py\u001b[0m in \u001b[0;36m_rmtree_unsafe\u001b[1;34m(path, onerror)\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m         \u001b[0mnames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[0monerror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'data_analyst_nanodegree_term_02_project_03_wrangle_report_v2_0_files'"
     ]
    }
   ],
   "source": [
    "#delete the images folder that is OUTSIDE the git_page folder\n",
    "shutil.rmtree(images_folder_name) #remove folder\n",
    "#delete the markdonw file that is OUTSIDE the git_page folder\n",
    "os.remove(markdown_name) #remove file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
